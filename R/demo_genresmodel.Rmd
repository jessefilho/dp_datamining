---
title: "Genre DEMO"
output: html_notebook
---

This is the demo for solve Genre problematic. In order to well understand this file was resumed to only show up the data partition and model test.

Main goal of this model:
- Classify new instances using these models;

Strategies:
- Find the must importantes features;
- Find the best model possible;
- Apply Supervised Classi fication;


Considerations:

- "Others" contains among genres known, anothers genres do not categorized.
- 


```{r}
#Load data
genres = read.csv(file = '~/Documents/dp_datamining/data/dataGenre.csv', header = TRUE, na.strings = "?")

#Data Preparation 
#Remove lyrics from this set
#genres$titlesong <- NULL
#Remove "tempo equals 0, because is not relevante for now."
genres <- genres[!genres$tempo <= 45.0 & !genres$tempo >= 250.0,]
#Remove "Others from type of group, because is not relevante now."
genres <- genres[!genres$typegroup == "Others",]

genres$Class[genres$genretype=='Others'] <- 0
genres$Class[genres$genretype=='Rock'] <- 1
genres$Class[genres$genretype=='Pop'] <- 1
genres$Class[genres$genretype=='Hip Hop'] <- 1
genres$Class[genres$genretype=='Jazz'] <- 1
genres$Class[genres$genretype=='Funk'] <- 1
genres$Class[genres$genretype=='Folk'] <- 1
genres$Class[genres$genretype=='Classical'] <- 1
genres$Class[genres$genretype=='Blues'] <- 1
genres$Class[genres$genretype=='Country'] <- 1
genres$Class[genres$genretype=='Electronic'] <- 1

#Create variables of genres
genres$Rock[genres$genretype=='Rock'] <- TRUE
genres$Rock[genres$genretype!='Rock'] <- FALSE

genres$Pop[genres$genretype=='Pop'] <- TRUE
genres$Pop[genres$genretype!='Pop'] <- FALSE

genres$Electronic[genres$genretype=='Electronic'] <- TRUE
genres$Electronic[genres$genretype!='Electronic'] <- FALSE

genres$HipHop[genres$genretype=='Hip Hop'] <- TRUE
genres$HipHop[genres$genretype!='Hip Hop'] <- FALSE

genres$Jazz[genres$genretype=='Jazz'] <- TRUE
genres$Jazz[genres$genretype!='Jazz'] <- FALSE

genres$Funk[genres$genretype=='Funk'] <- TRUE
genres$Funk[genres$genretype!='Funk'] <- FALSE

genres$Folk[genres$genretype=='Folk'] <- TRUE
genres$Folk[genres$genretype!='Folk'] <- FALSE

genres$Classical[genres$genretype=='Classical'] <- TRUE
genres$Classical[genres$genretype!='Classical'] <- FALSE

genres$Country[genres$genretype=='Country'] <- TRUE
genres$Country[genres$genretype!='Country'] <- FALSE

genres$Blues[genres$genretype=='Blues'] <- TRUE
genres$Blues[genres$genretype!='Blues'] <- FALSE



summary(genres$genretype)

summary(genres)
```

Steps to prepare data for the Classfication Model:
1. Reduction of the dataset of "Others" for take a proportion;
2. If necessary viabilization of a set for a 3th test


```{r}
library(dplyr)
library(quadprog)
#take a sample of genres "Others"
#Only 2% from attributes with the label Others
idx_others <- genres[sample(which(genres$genretype=='Others'),size = 0.008* nrow(genres[which(genres$genretype=='Others'),]) ), ]
#dataset without the label Others
dataset_withoutOthers <- genres[!genres$genretype=='Others',]
#Reattach A small and random data set with Others
genres_ToSplit <- bind_rows(idx_others,dataset_withoutOthers)

summary(genres_ToSplit)
```



2. Split data in 2 dataset:
2.1 Train
2.2 Test

```{r}
# Split between 80 and 30
index <- sample(1:nrow(genres_ToSplit),size = 0.7* nrow(genres_ToSplit))
# Use the large partition to training - 70
train <- genres_ToSplit[index,]
# Use the small partition to test - 30
test <- genres_ToSplit[-index,]
```


```{r}
summary(train)
summary(train$genretype)

```
```{r}
summary(test)
summary(test$genretype)
```
```{r}
print(train[,3:11])
```


Build the first classfication model with set of train:
- Decision Tree;
- Nayve Bayes

```{r}
library(caret)
library(ggplot2)
library(rpart.plot)
library(klaR)

# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"


# b) nb 
set.seed(7)
fit.nb_train <- train(genretype ~ +tempo+energy+danceability+speechiness+accoustiness+loudness+duration+valence+instrumentalness, data = train, method = "nb", metric=metric, trControl=control)

# c) knn 
set.seed(7)
fit.knn_train <- train(genretype ~ +tempo+energy+danceability+speechiness+accoustiness+loudness+duration+valence+instrumentalness, data = train, method = "knn", metric=metric, trControl=control)

# estimate variable importance
importance <- varImp(fit.knn_train, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)

# summarize accuracy of models
results <- resamples(list(nb=fit.nb_train, 
                          knn=fit.knn))
summary(results)

# compare accuracy of models
dotplot(results)


```
```{r}
library(rpart)
library(rpart.plot)
#Build the first classfication model
#to using the command rpat, R will build a tree where Class is to be predicted from the variable presents at the formula

#Model classfication of train
#Rock+tempo+energy+danceability+speechiness+accoustiness+loudness+loudness+duration+valence+instrumentalness
fit.tree_train = rpart(Class ~ -tempo+energy+danceability+speechiness+accoustiness+loudness+duration+valence+instrumentalness, data = train, method = "class")

#Plotting the tree of train
plot(fit.tree_train, compress=TRUE, uniform=TRUE)
text(fit.tree_train,use.n = T,all=T,cex=.7, pretty=0, xpd=TRUE, digits = 6)
prp(fit.tree_train, extra=101)
# Visualize the decision tree with rpart.plot
rpart.plot(fit.tree_train, box.palette="RdBu", shadow.col="gray", nn=TRUE)
#Test errors of train tree
pred <- predict(fit.tree_train, train, type = "class")
mConfusion <- table(train$Class, pred)
print(mConfusion)

#Test Accuracy of model
acc <- (mConfusion[1,1]+mConfusion[2,2])/sum(mConfusion)
print("acc")
print(acc*100)

#Compute error rate
Err <- (mConfusion[1,2]+mConfusion[2,1])/sum(mConfusion)
print("err")
print(Err*100)
```



```{r}
library(rpart)
library(rpart.plot)
#Build the first classfication model
#to using the command rpat, R will build a tree where Class is to be predicted from the variable presents at the formula

#Model classfication of train
#Rock+tempo+energy+danceability+speechiness+accoustiness+loudness+loudness+duration+valence+instrumentalness
trainTree_Rock = rpart(genretype ~ +Rock+tempo+energy+danceability+speechiness+accoustiness+loudness+duration+valence+instrumentalness, data = train, method = "class")

#Plotting the tree of train
plot(trainTree_Rock, compress=TRUE, uniform=TRUE)
text(trainTree_Rock,use.n = T,all=T,cex=.7, pretty=0, xpd=TRUE, digits = 6)
prp(trainTree_Rock, extra=101)
# Visualize the decision tree with rpart.plot
rpart.plot(trainTree_Rock, box.palette="RdBu", shadow.col="gray", nn=TRUE)
#Test errors of train tree
pred <- predict(trainTree_Rock, train, type = "class")
mConfusion <- table(train$Class, pred)
print(mConfusion)

#Test Accuracy of model
acc <- (mConfusion[1,1]+mConfusion[2,2])/sum(mConfusion)
print("acc")
print(acc*100)

#Compute error rate
Err <- (mConfusion[1,2]+mConfusion[2,1])/sum(mConfusion)
print("err")
print(Err*100)



```

Build the first classfication model with set of test
```{r}
#Build the first classfication model
#to using the command rpat, R will build a tree where Class is to be predicted from the variable presents at the formula


#Model classfication of train
testTree_Rock = rpart(Class ~ Rock+tempo+energy+danceability+speechiness+accoustiness+loudness+loudness+duration+valence+instrumentalness, data = test, method = "class")

#Plotting the tree of train
plot(testTree_Rock, compress=TRUE, uniform=TRUE)
text(testTree_Rock,use.n = T,all=T,cex=.7, pretty=0, xpd=TRUE, digits = 6)
prp(testTree_Rock, extra=101)
# Visualize the decision tree with rpart.plot
rpart.plot(testTree_Rock, box.palette="RdBu", shadow.col="gray", nn=TRUE)
#Test errors of train tree
pred <- predict(testTree_Rock, test, type = "class")
mConfusion <- table(test$Class, pred)
print(mConfusion)
#Test Accuracy of model
acc <- (mConfusion[1,1]+mConfusion[2,2])/sum(mConfusion)
print("acc")
print(acc*100)

#Compute error rate
Err <- (mConfusion[1,2]+mConfusion[2,1])/sum(mConfusion)
print("err")
print(Err*100)


```

```{r}
library(rpart)
#Errors array
errors <- numeric(0)
#Accuracies array
accs <- numeric(0)
#Randomly shuffle the data
yourdata<-genres_ToSplit[sample(nrow(genres_ToSplit)),]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(yourdata)),breaks=10,labels=FALSE)
#Perform 10 fold cross validation
for(i in 1:10){
  #Segement your data by fold using the which() function 
  testIndexes <- which(folds==i,arr.ind=TRUE)
  testData <- yourdata[testIndexes, ]
  trainData <- yourdata[-testIndexes, ]
  #Use the test and train data partitions however you desire...
  
  #Model classfication of train
  trainTree_Rock = rpart(Class ~tempo+energy+danceability+speechiness+accoustiness+loudness+loudness+duration+valence+instrumentalness, data = trainData, method = "class")
  
  #Plotting the tree of train
  #plot(trainTree_Rock, compress=TRUE, uniform=TRUE)
  #text(trainTree_Rock,use.n = T,all=T,cex=.7, pretty=0, xpd=TRUE, digits = 6)
  #prp(trainTree_Rock, extra=101)

  
  pred <- predict(trainTree_Rock, trainData, type = "class")
  mConfusion <- table(trainData$Class, pred)
  
  #Test Accuracy of model
  acc <- (mConfusion[1,1]+mConfusion[2,2])/sum(mConfusion)
  #print("acc")
  #print(acc*100)
  
  #Compute error rate
  Err <- (mConfusion[1,2]+mConfusion[2,1])/sum(mConfusion)
  #print("err")
  #print(Err*100)
  
  accs <- rbind(accs,acc)
  errors <- rbind(errors,Err)
}
```

Accuracy
Accuracy (ACC) is calculated as the number of all correct predictions divided by the total number of the dataset. The best accuracy is 1.0, whereas the worst is 0.0. It can also be calculated by 1 – ERR.


Error rate
Error rate (ERR) is calculated as the number of all incorrect predictions divided by the total number of the dataset. The best error rate is 0.0, whereas the worst is 1.0.




```{r}
meanAcc <- mean(accs)
meanError <- mean(errors)
print("Acc mean: ")
print(meanAcc)
print("Err mean: ")
print(meanError)
```

Depend of quality of data...

```{r}
#Construction of q naive Bayes classifier

#Getting started with Naive Bayes
#Install the package
#install.packages(“e1071”)
#Loading the library
library(e1071)
#Randomly shuffle the data
yourdata<-genres_ToSplit[sample(nrow(genres_ToSplit)),]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(yourdata)),breaks=10,labels=FALSE)
#Perform 10 fold cross validation
#Segement your data by fold using the which() function 
testIndexes <- which(folds==i,arr.ind=TRUE)
testData <- yourdata[testIndexes, ]
trainData <- yourdata[-testIndexes, ]
  
#Fitting the Naive Bayes model
Naive_Bayes_Model=naiveBayes(Rock~ Rock+tempo+energy+danceability+speechiness+accoustiness+loudness+loudness+duration+valence+instrumentalness, data=trainData)
#What does the model say? Print the model summary
Naive_Bayes_Model

pred <- predict(Naive_Bayes_Model, trainData)
mConfusion <- table(trainData$Class, pred)
print(mConfusion)
#Test Accuracy of model
acc <- (mConfusion[1,1]+mConfusion[2,2])/sum(mConfusion)
#print("acc")
#print(acc*100)

#Compute error rate
Err <- (mConfusion[1,2]+mConfusion[2,1])/sum(mConfusion)
#print("err")
#print(Err*100)

accs <- rbind(accs,acc)
errors <- rbind(errors,Err)
  

meanAcc <- mean(accs)
meanError <- mean(errors)
print("Acc mean: ")
print(meanAcc)
print("Err mean: ")
print(meanError)


```

```{r}
library(e1071)
#Errors array
errors <- numeric(0)
#Accuracies array
accs <- numeric(0)
#Randomly shuffle the data
yourdata<-genres_ToSplit[sample(nrow(genres_ToSplit)),]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(yourdata)),breaks=10,labels=FALSE)
#Perform 10 fold cross validation
for(i in 1:10){
  #Segement your data by fold using the which() function 
  testIndexes <- which(folds==i,arr.ind=TRUE)
  testData <- yourdata[testIndexes, ]
  trainData <- yourdata[-testIndexes, ]
  #Use the test and train data partitions however you desire...
  
  #Fitting the Naive Bayes model
  Naive_Bayes_Model=naiveBayes(Rock~ Rock+tempo+energy+danceability+speechiness+accoustiness+loudness+loudness+duration+valence+instrumentalness, data=trainData)
  #What does the model say? Print the model summary
  Naive_Bayes_Model
  
  #Plotting the tree of train
  #plot(trainTree_Rock, compress=TRUE, uniform=TRUE)
  #text(trainTree_Rock,use.n = T,all=T,cex=.7, pretty=0, xpd=TRUE, digits = 6)
  #prp(trainTree_Rock, extra=101)

  
  pred <- predict(Naive_Bayes_Model, trainData)
  mConfusion <- table(trainData$Class, pred)
  #print(mConfusion)
  #Test Accuracy of model
  acc <- (mConfusion[1,1]+mConfusion[2,2])/sum(mConfusion)
  #print("acc")
  #print(acc*100)
  
  #Compute error rate
  Err <- (mConfusion[1,2]+mConfusion[2,1])/sum(mConfusion)
  #print("err")
  #print(Err*100)
  
  accs <- rbind(accs,acc)
  errors <- rbind(errors,Err)
}

meanAcc <- mean(accs)
meanError <- mean(errors)
print("Acc mean: ")
print(meanAcc)
print("Err mean: ")
print(meanError)
```

```{r}
str(genres_ToSplit)
```


```{r}
set.seed(123)
library(randomForest)
#Randomly shuffle the data
yourdata<-genres_ToSplit[sample(nrow(genres_ToSplit)),]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(yourdata)),breaks=10,labels=FALSE)
#Perform 10 fold cross validation
#Segement your data by fold using the which() function 
testIndexes <- which(folds==i,arr.ind=TRUE)
testData <- yourdata[testIndexes, ]
trainData <- yourdata[-testIndexes, ]
fit <- randomForest(genretype ~ +tempo+energy+danceability+speechiness+accoustiness+loudness+duration+valence+instrumentalness, data = trainData, na.action = na.roughfix)
print(fit)


```

```{r}
varImpPlot(fit)
fit$importance
fit$importance[order(fit$importance[, 1], decreasing = TRUE), ]
```

```{r}
plot(genretype ~ accoustiness, data = trainData)
plot(genretype ~ duration, data = trainData)
```

```{r}
plot(fit$err.rate[, 1], type = "l", xlab = "nombre d'arbres", ylab = "erreur OOB")
```

```{r}
set.seed(123)
fit <- randomForest(genretype ~ +tempo+energy+danceability+speechiness+accoustiness+loudness+duration+valence+instrumentalness, data = trainData, ntree = 340, 
    mtry = 2, na.action = na.roughfix)
print(fit)
```

```{r}
set.seed(123)
library(caret)
mod <- train(genretype ~ +tempo+energy+danceability+speechiness+accoustiness+loudness+duration+valence+instrumentalness, data = trainData, method = "rf")
print(mod)
```

```{r}
set.seed(123)
library(caret)
mod <- train(genretype ~ +tempo+energy+danceability+speechiness+accoustiness+loudness+duration+valence+instrumentalness, data = trainData, method = "knn")
print(mod)
```



